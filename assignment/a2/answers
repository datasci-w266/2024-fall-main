# Write your short answers in this file, replacing the placeholders as appropriate.
# This assignment consists of 3 parts for a total of 79 points.
# For numerical answers, copy and paste at least 5 significant figures.
# - Neural Network DAN Text Classification (22 points)
# - WAN Text Classification (17 points)
# - BERT Text Classification (36 points)
# - Correct submission (2 point)
# - Answer file parses (2 point)



###################################################################
###################################################################
## Neural Network DAN Text Classification (22 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (1): Keras Functional API warm up (5 points)  | 
# ------------------------------------------------------------------

# Question 1.a (/5): I created a model using the Keras functional API that identically reproduces the model summary shown
# (This question is multiple choice.  Delete all but the correct answer).
neural_network_dan_text_classification_1_1_a: 
 - False
 - True


# ------------------------------------------------------------------
# | Section (1.1): Classification with various Word2Vec-based Models (2 points)  | 
# ------------------------------------------------------------------

# Question 1.1.a (/1): What is the percentage of positive examples in the training set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_a: 0.00000

# Question 1.1.b (/1): What is the percentage of positive examples in the test set (e.g. 72.575% is 0.72575)?
neural_network_dan_text_classification_1_1_1_1_b: 0.00000


# ------------------------------------------------------------------
# | Section (1.2): The Role of Shuffling of the Training Set (6 points)  | 
# ------------------------------------------------------------------

# Question 1.2.a (/3): What is the final validation accuracy that you observed after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_2_1_2_a: 0.00000

# Question 1.2.b (/3): What is the final validation accuracy that you observed for the shuffled run after 10 epochs?
neural_network_dan_text_classification_1_2_1_2_b: 0.00000


# ------------------------------------------------------------------
# | Section (1.3): Approaches for Training of Embeddings (9 points)  | 
# ------------------------------------------------------------------

# Question 1.3.a (/3): What is the final validation accuracy that you observed for the static model after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_a: 0

# Question 1.3.b (/3): What is the final validation accuracy that you observed for the model where you initialized with word2vec vectors but allow them to retrain for 3 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_b: 0

# Question 1.3.c (/3): What is the final validation accuracy that you observed for the model where you initialized randomly and then trained? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
neural_network_dan_text_classification_1_3_1_3_c: 0



###################################################################
###################################################################
## WAN Text Classification (17 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (2): Weighted Averaging Models using Attention (17 points)  | 
# ------------------------------------------------------------------

# Question 2.1.a (/2): Calculate the context vector for the following query and key/value vectors.
wan_text_classification_2_2_1_a: [d0, d1, d2]

# Question 2.1.b (/2): What are the weights for the key/value vectors?
wan_text_classification_2_2_1_b: [d0, d1]

# Question 2.2.a (/7): What is the final validation accuracy that you observed for the wan training after 10 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
wan_text_classification_2_2_2_a: 0

# Question 2.2.b (/3): List the 5 most important words separated by commas. (Again, if a word appears twice, include it twice.)
wan_text_classification_2_2_2_b: [d0, d1, d2, d3, d4]

# Question 2.2.c (/3): List the 5 least important words separated by commas. (Again, if a word appears twice, include it twice.)
wan_text_classification_2_2_2_c: [d0, d1, d2, d3, d4]



###################################################################
###################################################################
## BERT Text Classification (36 points)
###################################################################
###################################################################


# ------------------------------------------------------------------
# | Section (3.1): Tokenization with BERT (15 points)  | 
# ------------------------------------------------------------------

# Question 3.1.a (/1): Why do the attention_masks have 4 and 1 zeros, respectively?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_a: 
 - For the first example the last four tokens belong to a different segment. For the second one it is only the last token.
 - For the first example 4 positions are padded while for the second one it is only one.

# Question 3.1.b (/1): How many outputs are there?
bert_text_classification_3_1_3_1_b: 
- your answer

# Question 3.1.c (/1): Which output do we need to use to get token-level embeddings?
# (This question is multiple choice.  Delete all but the correct answer).
bert_text_classification_3_1_3_1_c: 
 - the first
 - the second

# Question 3.1.d (/2): Which input_id number corresponds to 'bank' in the two sentences?
bert_text_classification_3_1_3_1_d: 
- your answer

# Question 3.1.e (/2): Which token array index number corresponds to 'bank' in the first sentence?
bert_text_classification_3_1_3_1_e: 
- your answer

# Question 3.1.f (/2): Which array index number corresponds to 'bank' in the second sentence?
bert_text_classification_3_1_3_1_f: 
- your answer

# Question 3.1.g (/3): What is the cosine similarity between the BERT outputs for the two occurences of 'bank' in the two sentences?
bert_text_classification_3_1_3_1_g: 
- your answer

# Question 3.1.h (/3): How does this relate to the cosine similarity of 'this' (sentence 1) and 'the' (sentence 2). Compute the cosine similarity.
bert_text_classification_3_1_3_1_h: 
- your answer


# ------------------------------------------------------------------
# | Section (3.2): Classification with BERT (21 points)  | 
# ------------------------------------------------------------------

# Question 3.2.a (/7): What is the final validation accuracy that you observed for the BERT [CLS]-classification model after training for 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_2_a: 0

# Question 3.3.a (/7): What is the final validation accuracy that you observed for the BERT-averaging-classification model after training for 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_3_a: 0

# Question 3.4.a (/7): What is the final validation accuracy that you observed for the BERT-CNN-classification model after 2 epochs? (Copy and paste the decimal value e.g. a number like 0.5678 or 0.87654)
bert_text_classification_3_2_3_4_a: 0
